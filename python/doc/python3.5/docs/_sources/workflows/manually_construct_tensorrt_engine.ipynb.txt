{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Construct a TensorRT Engine\n",
    "\n",
    "With the release of UFF (Universal Framework Format), converting models from compatable frameworks to TensorRT engines is much easier. However, there maybe frameworks that do not currently have UFF exporters or never will. The Python API provides a path forward for Python based frameworks with it's numpy compatable layer weights. \n",
    "\n",
    "For this example we are going to be using PyTorch, and show how you can train a model then manually convert the model into a TensorRT engine. \n",
    "\n",
    "For python the TensorRT library is refered to as ```tensorrt```, for the Early Access you should have been provided a wheel file with the API, this can be installed by using ```pip``` (e.g. for python2.7 on Ubuntu 16.04- ```pip install tensorrt-3.0.2-cp27-cp27mu-linux_x86_64.whl```). For the Release Canidate forward you can also install the Python API with ```apt-get``` (```apt-get install python-tensorrt```)\n",
    "\n",
    "You can import tensorrt as you would import any other package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some common tools that are used with tensorrt typically. We use PyCUDA to handle the CUDA operations needed to allocate memory on your GPU and to transfer data to the GPU and results back to the CPU. We also use numpy as our primary method to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow #to show test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to import PyTorch and its various packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model in PyTorch\n",
    "\n",
    "We are going to move quickly through the PyTorch component of this example since it is not the focus. If you want to learn more about PyTorch and how to use it, check out http://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start out by setting some hyper parameters, then create a dataloader, define our network, set our optimizer and define our train and test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 1000\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "SGD_MOMENTUM = 0.5  \n",
    "SEED = 1\n",
    "LOG_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enable Cuda\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader  = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/mnist/data', train=True, download=True, \n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/mnist/data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(800, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(self.conv2(x), kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 800)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=SGD_MOMENTUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if batch % LOG_INTERVAL == 0:\n",
    "            #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "            #      .format(epoch, \n",
    "            #              batch * len(data), \n",
    "            #              len(train_loader.dataset), \n",
    "            #              100. * batch / len(train_loader), \n",
    "            #              loss.data[0]))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    test_loss /= len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, \n",
    "                  correct, \n",
    "                  len(test_loader.dataset), \n",
    "                  100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in range(EPOCHS):\n",
    "    train(e + 1)\n",
    "    test(e + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Model into a TensorRT Engine\n",
    "Now that we have a \"trained\" model we are going to start converting the model by first extract the layer wieghts by getting the ```state_dict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to start converting the model to TensorRT by first creating a builder and a logger for the build process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)\n",
    "builder = trt.infer.create_infer_builder(G_LOGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create the network by replicating the network structure above and extracting the weights in the form of numpy arrays from PyTorch. There are more elegant ways of doing this but we have expanded it out to show how it works. The numpy arrays from PyTorch reflect the dimensionality of the layers, so we reshape to flatten the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = builder.create_network()\n",
    "\n",
    "#Name for the input layer, data type, tuple for dimension \n",
    "data = network.add_input(\"data\", trt.infer.DataType.FLOAT, (1, 28, 28))\n",
    "assert(data)\n",
    "\n",
    "#-------------\n",
    "conv1_w = weights['conv1.weight'].cpu().numpy().reshape(-1)\n",
    "conv1_b = weights['conv1.bias'].cpu().numpy().reshape(-1)\n",
    "conv1 = network.add_convolution(data, 20, (5,5),  conv1_w, conv1_b)\n",
    "assert(conv1)\n",
    "conv1.set_stride((1,1))\n",
    "\n",
    "#-------------\n",
    "pool1 = network.add_pooling(conv1.get_output(0), trt.infer.PoolingType.MAX, (2,2))\n",
    "assert(pool1)\n",
    "pool1.set_stride((2,2))\n",
    "\n",
    "#-------------\n",
    "conv2_w = weights['conv2.weight'].cpu().numpy().reshape(-1)\n",
    "conv2_b = weights['conv2.bias'].cpu().numpy().reshape(-1)\n",
    "conv2 = network.add_convolution(pool1.get_output(0), 50, (5,5), conv2_w, conv2_b)\n",
    "assert(conv2)\n",
    "conv2.set_stride((1,1))\n",
    "\n",
    "#-------------\n",
    "pool2 = network.add_pooling(conv2.get_output(0), trt.infer.PoolingType.MAX, (2,2))\n",
    "assert(pool2)\n",
    "pool2.set_stride((2,2))\n",
    "\n",
    "#-------------\n",
    "fc1_w = weights['fc1.weight'].cpu().numpy().reshape(-1)\n",
    "fc1_b = weights['fc1.bias'].cpu().numpy().reshape(-1)\n",
    "fc1 = network.add_fully_connected(pool2.get_output(0), 500, fc1_w, fc1_b)\n",
    "assert(fc1)\n",
    "\n",
    "#-------------\n",
    "relu1 = network.add_activation(fc1.get_output(0), trt.infer.ActivationType.RELU)\n",
    "assert(relu1)\n",
    "\n",
    "#-------------\n",
    "fc2_w = weights['fc2.weight'].cpu().numpy().reshape(-1)\n",
    "fc2_b = weights['fc2.bias'].cpu().numpy().reshape(-1)\n",
    "fc2 = network.add_fully_connected(relu1.get_output(0), 10, fc2_w, fc2_b)\n",
    "assert(fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to mark our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc2.get_output(0).set_name(\"prob\")\n",
    "network.mark_output(fc2.get_output(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now are going to set the rest of the parameters for the network (max batch size and max workspace) and build the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "builder.set_max_batch_size(1)\n",
    "builder.set_max_workspace_size(1 << 20)\n",
    "\n",
    "engine = builder.build_cuda_engine(network)\n",
    "network.destroy()\n",
    "builder.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the engine runtime and generate a test case from the torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runtime = trt.infer.create_infer_runtime(G_LOGGER)\n",
    "img, target = next(iter(test_loader))\n",
    "img = img.numpy()[0]\n",
    "target = target.numpy()[0]\n",
    "%matplotlib inline\n",
    "img.shape\n",
    "imshow(img[0])\n",
    "print(\"Test Case: \" + str(target))\n",
    "img = img.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create an execution context for the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to now allocate the memory on the GPU and allocate memory on the CPU to hold results after inference. The size of the allocations is the size of the input and expected output * the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = np.empty(10, dtype = np.float32)\n",
    "\n",
    "#alocate device memory\n",
    "d_input = cuda.mem_alloc(1 * img.size * img.dtype.itemsize)\n",
    "d_output = cuda.mem_alloc(1 * output.size * output.dtype.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engine needs bindings provided as pointers to the GPU memory. PyCUDA lets us do this for memory allocations by casting those allocations to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bindings = [int(d_input), int(d_output)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also are going to create a cuda stream to run inference in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to transfer the data to the GPU, run inference and the copy the results back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transfer input data to device\n",
    "cuda.memcpy_htod_async(d_input, img, stream)\n",
    "#execute model \n",
    "context.enqueue(1, bindings, stream.handle, None)\n",
    "#transfer predictions back\n",
    "cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "#syncronize threads\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our results. We can just run ArgMax to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Test Case: \" + str(target))\n",
    "print (\"Prediction: \" + str(np.argmax(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save our engine to a file to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trt.utils.write_engine_to_file(\"./pyt_mnist.engine\", engine.serialize()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load this engine later by using ```tensorrt.utils.load_engine```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_engine = trt.utils.load_engine(G_LOGGER, \"./pyt_mnist.engine\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a final step, we are going to clean up our context, engine and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context.destroy()\n",
    "engine.destroy()\n",
    "new_engine.destroy()\n",
    "runtime.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
