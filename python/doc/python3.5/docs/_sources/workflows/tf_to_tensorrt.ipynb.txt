{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate TensorRT Engines from Tensorflow (or other UFF Compatable Frameworks)\n",
    "\n",
    "TensorRT 3.0.2 includes the UFF (Universal Framework Format) parser, a way to import UFF models and generate TensorRT engines. The UFF Toolkit which was released with TensorRT 3.0 provides support for converting Tensorflow models to UFF, there by allowing Tensorflow users to access the performace gains of TensorRT. With the Python API you can now go from training in Tensorflow to deploying in TensorRT without leaving Python. \n",
    "\n",
    "For this example, we are going to train a LeNet5 model to classify handwritten digits and then generate a TensorRT Engine for inference.  \n",
    "\n",
    "For python the TensorRT library is refered to as ```tensorrt```, for the Early Access you should have been provided a wheel file with the API, this can be installed by using ```pip``` (e.g. for python2.7 - ```pip install tensorrt-3.0.2-cp27-cp27mu-linux_x86_64.whl```). For the Release Canidate forward you can also install the Python API with ```apt-get``` (```apt-get install python-tensorrt```)\n",
    "\n",
    "We need to import Tensorflow and its various packages (note: there is a know bug in the EA where Tensorflow needs to be imported before TensorRT, this will addressed in the RC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd5f8e00c032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import TensorRT and its parsers like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt.parsers import uffparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some common tools that are used with tensorrt typically. We use PyCUDA to handle the CUDA operations needed to allocate memory on your GPU and to transfer data to the GPU and results back to the CPU. We also use numpy as our primary method to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from random import randint # generate a random test case\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow #to show test case\n",
    "import time #import system tools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to import the UFF toolkit to convert the graph from a serialized frozen tensorflow model to UFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import uff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model in Tensorflow \n",
    "\n",
    "We are going to move quickly through the Tensorflow component ofthis example since its not the focus. If you want to learn more about Tensorflow and how to use it, checkout https://www.tensorflow.org/get_started/get_started\n",
    "\n",
    "We are going to start by defining some hyper parameters, then defining some helper functions to make the code a bit less verbose. We will then define a network, then define our loss metrics, training and test steps our input nodes, and a data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STARTER_LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 10\n",
    "NUM_CLASSES = 10\n",
    "MAX_STEPS = 3000\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE ** 2\n",
    "OUTPUT_NAMES = [\"fc2/Relu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice that we are padding our Conv2d layer. TensorRT expects symetric padding for layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WeightsVariable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1, name='weights'))\n",
    "\n",
    "def BiasVariable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape, name='biases'))\n",
    "\n",
    "def Conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    filter_size = W.get_shape().as_list()\n",
    "    pad_size = filter_size[0]//2\n",
    "    pad_mat = np.array([[0,0],[pad_size,pad_size],[pad_size,pad_size],[0,0]])\n",
    "    x = tf.pad(x, pad_mat)\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def MaxPool2x2(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    pad_size = k//2\n",
    "    pad_mat = np.array([[0,0],[pad_size,pad_size],[pad_size,pad_size],[0,0]])\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(images):\n",
    "    # Convolution 1\n",
    "    with tf.name_scope('conv1'):\n",
    "        weights = WeightsVariable([5,5,1,32])\n",
    "        biases = BiasVariable([32])\n",
    "        conv1 = tf.nn.relu(Conv2d(images, weights, biases))\n",
    "        pool1 = MaxPool2x2(conv1)\n",
    "\n",
    "    # Convolution 2\n",
    "    with tf.name_scope('conv2'):\n",
    "        weights = WeightsVariable([5,5,32,64])\n",
    "        biases = BiasVariable([64])\n",
    "        conv2 = tf.nn.relu(Conv2d(pool1, weights, biases))\n",
    "        pool2 = MaxPool2x2(conv2)\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Fully Connected 1\n",
    "    with tf.name_scope('fc1'):\n",
    "        weights = WeightsVariable([7 * 7 * 64, 1024])\n",
    "        biases = BiasVariable([1024])\n",
    "        fc1 = tf.nn.relu(tf.matmul(pool2_flat, weights) + biases)\n",
    "\n",
    "    # Fully Connected 2\n",
    "    with tf.name_scope('fc2'):\n",
    "        weights = WeightsVariable([1024, 10])\n",
    "        biases = BiasVariable([10])\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases)\n",
    "\n",
    "    return fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_metrics(logits, labels):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, \n",
    "                                                                   logits=logits, \n",
    "                                                                   name='softmax')\n",
    "    return tf.reduce_mean(cross_entropy, name='softmax_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(STARTER_LEARNING_RATE, \n",
    "                                               global_step, \n",
    "                                               100000, \n",
    "                                               0.75, \n",
    "                                               staircase=True)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set,\n",
    "            summary):\n",
    "\n",
    "    true_count = 0\n",
    "    steps_per_epoch = data_set.num_examples // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder)\n",
    "        log, correctness = sess.run([summary, eval_correct], feed_dict=feed_dict)\n",
    "        true_count += correctness\n",
    "    precision = float(true_count) / num_examples\n",
    "    tf.summary.scalar('precision', tf.constant(precision))\n",
    "    print('Num examples %d, Num Correct: %d Precision @ 1: %0.04f' % \n",
    "          (num_examples, true_count, precision))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(None))\n",
    "    return images_placeholder, labels_placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    images_feed, labels_feed = data_set.next_batch(BATCH_SIZE)\n",
    "    feed_dict = {\n",
    "        images_pl: np.reshape(images_feed, (-1,28,28,1)),\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "    return feed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define our training pipeline in function that will return a frozen model with the training nodes removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(data_sets):\n",
    "    with tf.Graph().as_default():\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)\n",
    "        logits = network(images_placeholder)\n",
    "        loss = loss_metrics(logits, labels_placeholder)\n",
    "        train_op = training(loss)\n",
    "        eval_correct = evaluation(logits, labels_placeholder)\n",
    "        summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/tensorflow/mnist/log\", \n",
    "                                               graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(\"/tmp/tensorflow/mnist/log/validation\",  \n",
    "                                            graph=tf.get_default_graph())\n",
    "        sess.run(init)\n",
    "        for step in range(MAX_STEPS):\n",
    "            start_time = time.time()\n",
    "            feed_dict = fill_feed_dict(data_sets.train,\n",
    "                                       images_placeholder,\n",
    "                                       labels_placeholder)\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == MAX_STEPS:\n",
    "                checkpoint_file = os.path.join(\"/tmp/tensorflow/mnist/log\", \"model.ckpt\")\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "                print('Validation Data Eval:')\n",
    "                log = do_eval(sess,\n",
    "                              eval_correct,\n",
    "                              images_placeholder,\n",
    "                              labels_placeholder,\n",
    "                              data_sets.validation,\n",
    "                              summary)\n",
    "                test_writer.add_summary(log, step)\n",
    "        #return sess\n",
    "\n",
    "        graphdef = tf.get_default_graph().as_graph_def()\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(sess, \n",
    "                                                                    graphdef, \n",
    "                                                                    OUTPUT_NAMES)\n",
    "        return tf.graph_util.remove_training_nodes(frozen_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load the Tensorflow MNIST data loader and run training, the model does have summaries included so you can take at look at the training in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MNIST_DATASETS = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data')\n",
    "tf_model = run_training(MNIST_DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a Tensorflow Model to UFF\n",
    "We are now going to convert it into a serialized UFF model. To convert a model we need to at least provide the model stream and the name(s) of the desired output node(s). The UFF Toolkit also includes a ```uff.from_tensorflow_frozen_model``` function which takes a path to a frozen Tensorflow graph protobuf file. Both utilities have options for:\n",
    "- ```quiet``` mode to suppress conversion logging\n",
    "- ```input_nodes``` to allow you to define a set of input nodes in the graph (the defaults are Placeholders nodes)\n",
    "- ```text``` will let you save a human readable version of UFF model aloneside the binary UFF\n",
    "- ```list_nodes``` will list the nodes in the graph \n",
    "- ```output_filename``` will if provided write the model out to the filepath specified instead of returning a serialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uff_model = uff.from_tensorflow(tf_model, [\"fc2/Relu\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import a UFF Model into TensorRT and Create an Engine \n",
    "\n",
    "We now have a UFF modelsteam we can generate a TensorRT engine with. We are going to start by creating a logger for TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create a uff parser and identifying the desired input and output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = uffparser.create_uff_parser()\n",
    "parser.register_input(\"Placeholder\", (1,28,28), 0)\n",
    "parser.register_output(\"fc2/Relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to pass the logger, parser and the uff model stream and some settings (max batch size and max workspace size) to a utility function that will create the engine for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser, 1, 1 << 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to allocate some memory on the CPU to use while we have an active engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get rid of the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to get a test case from the Tensorflow dataloader (converting it to FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img, label = MNIST_DATASETS.test.next_batch(1)\n",
    "img = img[0]\n",
    "#convert input data to Float32\n",
    "img = img.astype(np.float32)\n",
    "label = label[0]\n",
    "%matplotlib inline\n",
    "imshow(img.reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create a runtime and an execution context for the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runtime = trt.infer.create_infer_runtime(G_LOGGER)\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to now allocate the memory on the GPU and allocate memory on the CPU to hold results after inference. The size of the allocations is the size of the input and expected output * the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = np.empty(10, dtype = np.float32)\n",
    "\n",
    "#alocate device memory\n",
    "d_input = cuda.mem_alloc(1 * img.size * img.dtype.itemsize)\n",
    "d_output = cuda.mem_alloc(1 * output.size * output.dtype.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engine needs bindings provided as pointers to the GPU memory. PyCUDA lets us do this for memory allocations by casting those allocations to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bindings = [int(d_input), int(d_output)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also are going to create a cuda stream to run inference in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to transfer the data to the GPU, run inference and the copy the results back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transfer input data to device\n",
    "cuda.memcpy_htod_async(d_input, img, stream)\n",
    "#execute model \n",
    "context.enqueue(1, bindings, stream.handle, None)\n",
    "#transfer predictions back\n",
    "cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "#syncronize threads\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our results. We can just run ArgMax to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Test Case: \" + str(label))\n",
    "print (\"Prediction: \" + str(np.argmax(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save our engine to a file to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trt.utils.write_engine_to_file(\"./tf_mnist.engine\", engine.serialize()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load this engine later by using ```tensorrt.utils.load_engine```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_engine = trt.utils.load_engine(G_LOGGER, \"./tf_mnist.engine\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a final step, we are going to clean up our context, engine and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context.destroy()\n",
    "engine.destroy()\n",
    "new_engine.destroy()\n",
    "runtime.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
