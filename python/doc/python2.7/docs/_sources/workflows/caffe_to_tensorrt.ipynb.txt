{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorRT to Optimize Caffe Models in Python\n",
    "\n",
    "TensorRT 3.0.2 includes support for a Python API to load in and optimize Caffe models which can then be executed and stored as portable PLAN files. The following notebook explains the workflow you can use to do so. \n",
    "\n",
    "For python the TensorRT library is refered to as ```tensorrt```, for the Early Access you should have been provided a wheel file with the API, this can be installed by using ```pip``` (e.g. for python2.7 on ubuntu 16.04 - ```pip install tensorrt-3.0.2-cp27-cp27mu-linux_x86_64.whl```). \n",
    "\n",
    "You can import tensorrt as you would import any other package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some common tools that are used with tensorrt typically. We use PyCUDA to handle the CUDA operations needed to allocate memory on your GPU and to transfer data to the GPU and results back to the CPU. We also use numpy as our primary method to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we also want to import an image processing library (```pillow``` in this case) and ```randint```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow #to show test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are converting a Caffe model we also need to use the ```caffeparser``` which is located in ```tensorrt.parsers```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorrt import parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the first thing you will do is create a logger, which is used in may places during the model conversion and inference process. We provide a simple logger implementation in ```tensorrt.infer.ConsoleLogger``` but in the RC you will be able to define your own as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define some constants about our model, which in this example we will used to classify digits from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_LAYERS = ['data']\n",
    "OUTPUT_LAYERS = ['prob']\n",
    "INPUT_H = 28\n",
    "INPUT_W =  28\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to define some paths, please change these to reflect where you placed the data included with the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_PROTOTXT = './data/mnist/mnist.prototxt'\n",
    "CAFFE_MODEL = './data/mnist/mnist.caffemodel'\n",
    "DATA = './data/mnist/'\n",
    "IMAGE_MEAN = './data/mnist/mnist_mean.binaryproto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create our engine. The Python API provides some nice utilities to make this much simplier. Here we use the caffe model converter utility in ```tensorrt.utils```. We provide it a logger, a path to the model prototxt, the model file, the max batch size, the max workspace size, the output layer(s) and the data type of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = trt.utils.caffe_to_trt_engine(G_LOGGER,\n",
    "                                       MODEL_PROTOTXT,\n",
    "                                       CAFFE_MODEL,\n",
    "                                       1, \n",
    "                                       1 << 20, \n",
    "                                       OUTPUT_LAYERS,\n",
    "                                       trt.infer.DataType.FLOAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a test case for our engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_file = randint(0,9)\n",
    "path = DATA + str(rand_file) + '.pgm'\n",
    "im = Image.open(path)\n",
    "%matplotlib inline\n",
    "imshow(np.asarray(im))\n",
    "arr = np.array(im)\n",
    "img = arr.ravel()\n",
    "print(\"Test Case: \" + str(rand_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to apply the mean to the input image, we have this stored in a .binaryproto file which we use the caffeparser to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = parsers.caffeparser.create_caffe_parser()\n",
    "mean_blob = parser.parse_binary_proto(IMAGE_MEAN)\n",
    "parser.destroy()\n",
    "#NOTE: This is different than the C++ API, you must provide the size of the data\n",
    "mean = mean_blob.get_data(INPUT_W ** 2) \n",
    "data = np.empty([INPUT_W ** 2])\n",
    "for i in range(INPUT_W ** 2):\n",
    "    data[i] = float(img[i]) - mean[i]\n",
    "mean_blob.destroy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a runtime for Inference and create a context for our engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runtime = trt.infer.create_infer_runtime(G_LOGGER)\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run inference, we are going to start by making sure our data is in the correct datatype (FP32 for this model). Then we are going to create an empty array on the CPU to hold our results from inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(engine.get_nb_bindings() == 2)\n",
    "#convert input data to Float32\n",
    "img = img.astype(np.float32)\n",
    "#create output array to receive data \n",
    "output = np.empty(OUTPUT_SIZE, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to allocate memory on the GPU with PyCUDA and register them with the engine. The size of the allocations is the size of the input and expected output * the batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_input = cuda.mem_alloc(1 * img.size * img.dtype.itemsize)\n",
    "d_output = cuda.mem_alloc(1 * output.size * output.dtype.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engine needs bindings provided as pointers to the GPU memory. PyCUDA lets us do this for memory allocations by casting those allocations to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bindings = [int(d_input), int(d_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also are going to create a cuda stream to run inference in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to transfer the data to the GPU, run inference and the copy the results back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transfer input data to device\n",
    "cuda.memcpy_htod_async(d_input, img, stream)\n",
    "#execute model \n",
    "context.enqueue(1, bindings, stream.handle, None)\n",
    "#transfer predictions back\n",
    "cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "#syncronize threads\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our results. We can just run ArgMax to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Test Case: \" + str(rand_file))\n",
    "print (\"Prediction: \" + str(np.argmax(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save our engine to a file to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trt.utils.write_engine_to_file(\"./data/mnist/new_mnist.engine\", engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load this engine later by using ```tensorrt.utils.load_engine```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_engine = trt.utils.load_engine(G_LOGGER, \"./data/mnist/new_mnist.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a final step, we are going to clean up our context, engine and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context.destroy()\n",
    "engine.destroy()\n",
    "new_engine.destroy()\n",
    "runtime.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
